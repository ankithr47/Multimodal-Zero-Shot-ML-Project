{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cbbf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_brain_seen, all_image_seen, all_text_seen, all_label_seen = [], [], [], []\n",
    "all_brain_unseen, all_image_unseen, all_text_unseen, all_label_unseen = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5117a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_brain_samples= 49620 , seen_brain_features= 561\n",
      "seen_image_samples= 49620 , seen_image_features= 100\n",
      "seen_text_samples= 49620 , seen_text_features= 512\n",
      "seen_label= torch.Size([49620, 1])\n",
      "unseen_brain_samples= 48000 , unseen_brain_features= 561\n",
      "unseen_image_samples= 48000 , unseen_image_features= 100\n",
      "unseen_text_samples= 48000 , unseen_text_features= 512\n",
      "unseen_label= torch.Size([48000, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load data\n",
    "import mmbra\n",
    "import mmbracategories\n",
    "import torch\n",
    "import os\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import mmbra\n",
    "import mmbracategories\n",
    "import torch\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "data_dir_root = os.path.join('./data', 'ThingsEEG-Text')\n",
    "\n",
    "sbj_list = ['sub-01', 'sub-02', 'sub-03']\n",
    "\n",
    "image_model = 'pytorch/cornet_s'\n",
    "text_model = 'CLIPText'\n",
    "roi = '17channels'\n",
    "\n",
    "for sbj in sbj_list:\n",
    "    brain_dir = os.path.join(data_dir_root, 'brain_feature', roi, sbj)\n",
    "    image_dir_seen = os.path.join(data_dir_root, 'visual_feature/ThingsTrain', image_model, sbj)\n",
    "    image_dir_unseen = os.path.join(data_dir_root, 'visual_feature/ThingsTest', image_model, sbj)\n",
    "    text_dir_seen = os.path.join(data_dir_root, 'textual_feature/ThingsTrain/text', text_model, sbj)\n",
    "    text_dir_unseen = os.path.join(data_dir_root, 'textual_feature/ThingsTest/text', text_model, sbj)\n",
    "\n",
    "    # ---- seen ----\n",
    "    brain_seen = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['data'].astype('double') * 2.0\n",
    "    brain_seen = brain_seen[:, :, 27:60]\n",
    "    brain_seen = np.reshape(brain_seen, (brain_seen.shape[0], -1))\n",
    "\n",
    "    image_seen = sio.loadmat(os.path.join(image_dir_seen, 'feat_pca_train.mat'))['data'].astype('double') * 50.0\n",
    "    image_seen = image_seen[:, 0:100]\n",
    "\n",
    "    text_seen = sio.loadmat(os.path.join(text_dir_seen, 'text_feat_train.mat'))['data'].astype('double') * 2.0\n",
    "\n",
    "    label_seen = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['class_idx'].T.astype('int')\n",
    "\n",
    "    # ---- unseen ----\n",
    "    brain_unseen = sio.loadmat(os.path.join(brain_dir, 'eeg_test_data.mat'))['data'].astype('double') * 2.0\n",
    "    brain_unseen = brain_unseen[:, :, 27:60]\n",
    "    brain_unseen = np.reshape(brain_unseen, (brain_unseen.shape[0], -1))\n",
    "\n",
    "    image_unseen = sio.loadmat(os.path.join(image_dir_unseen, 'feat_pca_test.mat'))['data'].astype('double') * 50.0\n",
    "    image_unseen = image_unseen[:, 0:100]\n",
    "\n",
    "    text_unseen = sio.loadmat(os.path.join(text_dir_unseen, 'text_feat_test.mat'))['data'].astype('double') * 2.0\n",
    "\n",
    "    label_unseen = sio.loadmat(os.path.join(brain_dir, 'eeg_test_data.mat'))['class_idx'].T.astype('int')\n",
    "\n",
    "    # collect\n",
    "    all_brain_seen.append(brain_seen)\n",
    "    all_image_seen.append(image_seen)\n",
    "    all_text_seen.append(text_seen)\n",
    "    all_label_seen.append(label_seen)\n",
    "\n",
    "    all_brain_unseen.append(brain_unseen)\n",
    "    all_image_unseen.append(image_unseen)\n",
    "    all_text_unseen.append(text_unseen)\n",
    "    all_label_unseen.append(label_unseen)\n",
    "\n",
    "# stack across subjects\n",
    "brain_seen  = torch.from_numpy(np.vstack(all_brain_seen))\n",
    "image_seen  = torch.from_numpy(np.vstack(all_image_seen))\n",
    "text_seen   = torch.from_numpy(np.vstack(all_text_seen))\n",
    "label_seen  = torch.from_numpy(np.vstack(all_label_seen))\n",
    "\n",
    "brain_unseen = torch.from_numpy(np.vstack(all_brain_unseen))\n",
    "image_unseen = torch.from_numpy(np.vstack(all_image_unseen))\n",
    "text_unseen  = torch.from_numpy(np.vstack(all_text_unseen))\n",
    "label_unseen = torch.from_numpy(np.vstack(all_label_unseen))\n",
    "\n",
    "print('seen_brain_samples=', brain_seen.shape[0], ', seen_brain_features=', brain_seen.shape[1])\n",
    "print('seen_image_samples=', image_seen.shape[0], ', seen_image_features=', image_seen.shape[1])\n",
    "print('seen_text_samples=', text_seen.shape[0], ', seen_text_features=', text_seen.shape[1])\n",
    "print('seen_label=', label_seen.shape)\n",
    "\n",
    "print('unseen_brain_samples=', brain_unseen.shape[0], ', unseen_brain_features=', brain_unseen.shape[1])\n",
    "print('unseen_image_samples=', image_unseen.shape[0], ', unseen_image_features=', image_unseen.shape[1])\n",
    "print('unseen_text_samples=', text_unseen.shape[0], ', unseen_text_features=', text_unseen.shape[1])\n",
    "print('unseen_label=', label_unseen.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db215f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   \\nTo ensure a strict zero-shot learning setup, we perform a \\nclass-level split of the label space into disjoint seen and unseen sets. \\nAll model development, including embedding refinement, \\nuses only samples from seen classes, while evaluation is performed exclusively \\non unseen classes. This prevents information leakage and aligns with the formal \\ndefinition of zero-shot learning.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''   \n",
    "To ensure a strict zero-shot learning setup, we perform a \n",
    "class-level split of the label space into disjoint seen and unseen sets. \n",
    "All model development, including embedding refinement, \n",
    "uses only samples from seen classes, while evaluation is performed exclusively \n",
    "on unseen classes. This prevents information leakage and aligns with the formal \n",
    "definition of zero-shot learning.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611a0c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes: 1654\n",
      "Seen classes: 1323\n",
      "Unseen classes: 331\n",
      "Final zero-shot split:\n",
      "Seen samples: 39690\n",
      "Unseen samples: 11520\n"
     ]
    }
   ],
   "source": [
    "#to avoid data leakage, we must split by classes (80% seen, 20% unseen)\n",
    "# ============================\n",
    "# ZERO-SHOT CLASS-LEVEL SPLIT\n",
    "# ============================\n",
    "\n",
    "# flatten labels to 1D numpy arrays\n",
    "y_seen_all = label_seen.numpy().reshape(-1)\n",
    "y_unseen_all = label_unseen.numpy().reshape(-1)\n",
    "\n",
    "all_classes = np.unique(np.concatenate([y_seen_all, y_unseen_all]))\n",
    "\n",
    "print(f\"Total number of classes: {len(all_classes)}\")\n",
    "\n",
    "#reproducible split\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(all_classes)\n",
    "\n",
    "n_seen_classes = int(0.8 * len(all_classes))\n",
    "seen_classes = all_classes[:n_seen_classes]\n",
    "unseen_classes = all_classes[n_seen_classes:]\n",
    "\n",
    "print(f\"Seen classes: {len(seen_classes)}\")\n",
    "print(f\"Unseen classes: {len(unseen_classes)}\")\n",
    "\n",
    "#create masks, ensure no leakage\n",
    "\n",
    "seen_mask = np.isin(y_seen_all, seen_classes)\n",
    "unseen_mask = np.isin(y_unseen_all, unseen_classes)\n",
    "\n",
    "#final zero-shot datasets\n",
    "\n",
    "X_seen = brain_seen[seen_mask]\n",
    "y_seen = label_seen[seen_mask]\n",
    "\n",
    "X_unseen = brain_unseen[unseen_mask]\n",
    "y_unseen = label_unseen[unseen_mask]\n",
    "\n",
    "print(\"Final zero-shot split:\")\n",
    "print(\"Seen samples:\", X_seen.shape[0])\n",
    "print(\"Unseen samples:\", X_unseen.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f7ba2",
   "metadata": {},
   "source": [
    "## Creating baseline zero-shot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3880773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG test shape: (11520, 512)\n"
     ]
    }
   ],
   "source": [
    "#EEG features (unseen only, as baseline does not train)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "X_seenNP = X_seen.numpy()\n",
    "X_unseenNP = X_unseen.numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_seen_scaled = scaler.fit_transform(X_seenNP)\n",
    "X_unseen_scaled = scaler.transform(X_unseenNP)\n",
    "\n",
    "#project EEG to 512 dims to match text (fit on seen only)\n",
    "pca = PCA(n_components=512, random_state=0)\n",
    "X_seen_512 = pca.fit_transform(X_seen_scaled)\n",
    "X_test = pca.transform(X_unseen_scaled)\n",
    "\n",
    "print(\"EEG test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e185d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen classes (in eval set): 48\n"
     ]
    }
   ],
   "source": [
    "#flatten labels\n",
    "y_unseenNP = y_unseen.numpy().reshape(-1).astype(int)\n",
    "\n",
    "unseen_classes = np.unique(y_unseenNP)\n",
    "print('Unseen classes (in eval set):', len(unseen_classes))\n",
    "\n",
    "#semantic prototypes is a dict {class_id : vector}\n",
    "#mean text embedding per unseen class for the prototype vector for each class\n",
    "text_unseen_filtered = text_unseen[unseen_mask]\n",
    "text_unseen_np = text_unseen_filtered.numpy()\n",
    "\n",
    "\n",
    "semantic_proto = {}\n",
    "for i in unseen_classes:\n",
    "    idx = np.where(y_unseenNP == i)[0]\n",
    "    semantic_proto[i] = text_unseen_np[idx[0]] #first instance for no peaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6cadded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity inference\n",
    "from numpy.linalg import norm\n",
    "\n",
    "#stack prototypes into matrix\n",
    "proto_label = list(semantic_proto.keys())\n",
    "proto_matrix = np.stack([semantic_proto[i] for i in proto_label])\n",
    "\n",
    "#normalise for cosine\n",
    "Xn = X_test / (np.linalg.norm(X_test, axis=1, keepdims=True)+1e-8)\n",
    "Pn = proto_matrix / (np.linalg.norm(proto_matrix, axis=1, keepdims=True)+1e-8)\n",
    "\n",
    "#cosine similarities: (N_unseen, C_unseen)\n",
    "\n",
    "S = Xn @ Pn.T\n",
    "\n",
    "y_pred = np.array([proto_label[j] for j in np.argmax(S, axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b4891ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot baseline accuracy: 0.0191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_unseenNP, y_pred)\n",
    "print(f\"Zero-shot baseline accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb8728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
